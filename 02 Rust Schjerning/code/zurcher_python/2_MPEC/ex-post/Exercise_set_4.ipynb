{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magics: ensures that any changes to the modules loaded below will be re-loaded automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext line_profiler\n",
    "\n",
    "# load general packages\n",
    "import numpy as np\n",
    "\n",
    "# load modules related to this exercise\n",
    "from  matplotlib.pyplot import spy\n",
    "from model_zucher import zurcher\n",
    "import Estimate_MPEC as estimate_MPEC\n",
    "\n",
    "from Solve_NFXP import solve_NFXP\n",
    "import estimate_NFXP as estimate_NFXP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise set 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "do_settings = {\n",
    "    'n': 90\n",
    "}\n",
    "\n",
    "model = zurcher(**do_settings)\n",
    "solver = solve_NFXP()\n",
    "\n",
    "#  SIMULATE DATA\n",
    "N = 500\n",
    "T = 120\n",
    "ev, pk = solver.poly(model.bellman, beta=model.beta, output=2)\n",
    "data = zurcher.sim_data(model,N,T,pk) \n",
    "samplesize = data.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Run the function mpec.sparsity_pattern.\n",
    "The function mpec.sparsity_pattern creates sparse matrices of indicators for where there are elements in the Jacobian of the constraints and Hessian of the likelihood function\n",
    "\n",
    "(a) Look at the figures, and talk about what the different elements of the Jacobian of the constraint and Hessian of the likelihood represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of parameter to be estimated\n",
    "Nc = 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "J_pattern, H_pattern = estimate_MPEC.sparsity_pattern(Nc,model.n, len(model.p)+1)\n",
    "\n",
    "# Figure\n",
    "fig = plt.figure(figsize=(20,5))# figsize is in inches...\n",
    "\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.spy(J_pattern,markersize=5)\n",
    "ax.set_title(f'Jacobian of constraints')\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "ax.spy(H_pattern,markersize=5)\n",
    "ax.set_title(f'Hessian of likelihood')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Why is it important that we handle the Jacobian and Hessian as sparse matrices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Estimate the model using MPEC. In order to estimate the model, you should understand:\n",
    "<il type =\"a\">\n",
    "<li> Estimate_MPEC.estimate </li>\n",
    "<li> Estimate_MPEC.ll (don't spend too much time on understanding the gradient)</li>\n",
    "<li> Estimate_MPEC.con_bellman (don't focus too much on computing Jacobian) </li>\n",
    "</il>\n",
    "\n",
    "Note that we in the implemenation don't use the information that the Hessian is sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Fill in the missing stuff in mpec.ll and mpec.con_bellman, and run the code to check that your results are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "theta0 = [11,2]\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "res_MPEC, pnames, theta_hat_MPEC = estimate_MPEC.estimate(model,data, theta0=theta0, twostep=1)\n",
    "\n",
    "#res_MPEC, pnames, theta_hat_MPEC = estimate_MPEC.estimate(model,data.loc[0:6000],theta0=theta0, twostep=1)\n",
    "t1 = time.time()\n",
    "time_MPEC=t1-t0\n",
    "\n",
    "# Print the results\n",
    "print(f'Structual estimation using busdata from Rust(1987)')\n",
    "print(f'Beta        = {model.beta:.4f}')\n",
    "print(f'n           = {model.n}')\n",
    "print(f'Sample size = {data.shape[0]}\\n \\n') \n",
    "\n",
    "print(f'Parameters     Estimates    s.e. ') \n",
    "print(f'{pnames[0]}             {theta_hat_MPEC[0]:.4f}     ')\n",
    "print(f'{pnames[1]}              {theta_hat_MPEC[1]:.4f}      \\n ')\n",
    "\n",
    "\n",
    "print(f'Log-likelihood {-res_MPEC.fun*samplesize:.4f}')  \n",
    "print(f'runtime (seconds) {time_MPEC:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%lprun -f estimate_MPEC.ll  -f estimate_MPEC.estimate -f estimate_MPEC.con_bellman -f estimate_MPEC.constraint_jac estimate_MPEC.estimate(model,data,theta0=theta0, twostep=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Compare NFXP and MPEC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve by NFXP\n",
    "\n",
    "t0 = time.time()\n",
    "nfxp_model, nfxp_results, pnames, theta_hat_NFXP, Avar_NFXP, converged=estimate_NFXP.estimate(model, solver, data, theta0=theta0, twostep=1)\n",
    "t1 = time.time()\n",
    "time_NFXP=t1-t0\n",
    "\n",
    "\n",
    "#compare the results \n",
    "print(f'Structual estimation using busdata from Rust(1987) \\n')\n",
    "\n",
    "print(f'MPEC')\n",
    "print(f'Parameters     Estimates    s.e. ') \n",
    "print(f'{pnames[0]}             {theta_hat_MPEC[0]:.4f}     ')\n",
    "print(f'{pnames[1]}              {theta_hat_MPEC[1]:.4f}      \\n ')\n",
    "\n",
    "\n",
    "print(f'Log-likelihood {-res_MPEC.fun*samplesize:.2f}')\n",
    "print(f'runtime (seconds) {time_MPEC:.4f}\\n \\n')\n",
    "\n",
    "\n",
    "print(f'NFXP')\n",
    "print(f'Parameters     Estimates    s.e. ') \n",
    "print(f'{pnames[0]}             {theta_hat_NFXP[0]:.4f}     {np.sqrt(Avar_NFXP[0,0]):.4f}      ')\n",
    "print(f'{pnames[1]}              {theta_hat_NFXP[1]:.4f}      {np.sqrt(Avar_NFXP[1,1]):.4f}       \\n ')\n",
    "\n",
    "\n",
    "print(f'Log-likelihood {-nfxp_results.fun*samplesize:.2f}')\n",
    "print(f'runtime (seconds) {time_NFXP:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Compare the time of NFXP and the time of MPEC, and the time of NFXP and MPEC from the lecture. According to what you saw at the lectures the two methods should be comparable with regards to speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Beta        = {model.beta:.4f}')\n",
    "print(f'n           = {model.n}')\n",
    "\n",
    "\n",
    "%timeit estimate_NFXP.estimate(model, solver, data, theta0=theta0, twostep=1)\n",
    "%timeit estimate_MPEC.estimate(model,data,theta0=theta0, twostep=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Do we use analytical first-order derivatives? \n",
    "\n",
    "(c) What about second-order derivatives? \n",
    "\n",
    "(d) What do they do in Su and Judd (2012)? \n",
    "\n",
    "(e) Why is our implementation inefficient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. How did we get our standard errors using NFXP? How would you calculate them using MPEC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
